{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nerfstudio-project/gsplat/issues/234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import struct\n",
    "from gsplat import rasterization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of ply file\n",
    "file_path = os.path.abspath(os.path.join(\"..\", \"data\", \"point_cloud.ply\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLY Header:\n",
      "ply\n",
      "format binary_little_endian 1.0\n",
      "element vertex 899421\n",
      "property float x\n",
      "property float y\n",
      "property float z\n",
      "property float nx\n",
      "property float ny\n",
      "property float nz\n",
      "property float f_dc_0\n",
      "property float f_dc_1\n",
      "property float f_dc_2\n",
      "property float f_rest_0\n",
      "property float f_rest_1\n",
      "property float f_rest_2\n",
      "property float f_rest_3\n",
      "property float f_rest_4\n",
      "property float f_rest_5\n",
      "property float f_rest_6\n",
      "property float f_rest_7\n",
      "property float f_rest_8\n",
      "property float f_rest_9\n",
      "property float f_rest_10\n",
      "property float f_rest_11\n",
      "property float f_rest_12\n",
      "property float f_rest_13\n",
      "property float f_rest_14\n",
      "property float f_rest_15\n",
      "property float f_rest_16\n",
      "property float f_rest_17\n",
      "property float f_rest_18\n",
      "property float f_rest_19\n",
      "property float f_rest_20\n",
      "property float f_rest_21\n",
      "property float f_rest_22\n",
      "property float f_rest_23\n",
      "property float f_rest_24\n",
      "property float f_rest_25\n",
      "property float f_rest_26\n",
      "property float f_rest_27\n",
      "property float f_rest_28\n",
      "property float f_rest_29\n",
      "property float f_rest_30\n",
      "property float f_rest_31\n",
      "property float f_rest_32\n",
      "property float f_rest_33\n",
      "property float f_rest_34\n",
      "property float f_rest_35\n",
      "property float f_rest_36\n",
      "property float f_rest_37\n",
      "property float f_rest_38\n",
      "property float f_rest_39\n",
      "property float f_rest_40\n",
      "property float f_rest_41\n",
      "property float f_rest_42\n",
      "property float f_rest_43\n",
      "property float f_rest_44\n",
      "property float opacity\n",
      "property float scale_0\n",
      "property float scale_1\n",
      "property float scale_2\n",
      "property float rot_0\n",
      "property float rot_1\n",
      "property float rot_2\n",
      "property float rot_3\n",
      "end_header\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    header_lines = []\n",
    "    for line in f:\n",
    "        decoded_line = line.decode('utf-8', errors='ignore').strip()\n",
    "        header_lines.append(decoded_line)\n",
    "        if decoded_line == \"end_header\":\n",
    "            break\n",
    "\n",
    "print(\"PLY Header:\")\n",
    "print(\"\\n\".join(header_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read ply file\n",
    "# def load_ply(file_path):\n",
    "#     \"\"\"\n",
    "#     Load a 3dgs PLY file and return a dictionary of torch.Tensors\n",
    "#     containing Gaussian parameters.\n",
    "\n",
    "#     The expected property order is as follows:\n",
    "#       1. x, y, z          -> means, shape [N, 3]\n",
    "#       2. nx, ny, nz       -> normals (unused, written as zeros)\n",
    "#       3. Color block: Either:\n",
    "#          - Plain colors: properties \"f_dc_0\", \"f_dc_1\", ..., representing\n",
    "#            transformed colors, or\n",
    "#          - SH coefficients: two blocks; first block (\"f_dc_0\", …) and\n",
    "#            second block (\"f_rest_0\", …). In this case, if each block has M floats,\n",
    "#            we require M to be divisible by 3. Then the SH coefficients are recovered\n",
    "#            by reshaping each block to [N, K, 3] with K = M / 3.\n",
    "#       4. opacity          -> opacities, shape [N]\n",
    "#       5. scale_*          -> scales, shape [N, 3]\n",
    "#       6. rot_*            -> quaternions, shape [N, 4]\n",
    "\n",
    "#     Returns a dictionary with keys:\n",
    "#       - \"means\", \"quats\", \"scales\", \"opacities\"\n",
    "#       - If plain colors are used: \"colors\" of shape [N, D]\n",
    "#       - If SH coefficients are used: \"sh0\" and \"shN\", each of shape [N, K, 3]\n",
    "#     \"\"\"\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         # Read header lines until \"end_header\"\n",
    "#         header_lines = []\n",
    "#         while True:\n",
    "#             line = f.readline().decode(\"utf-8\").strip()\n",
    "#             header_lines.append(line)\n",
    "#             if line == \"end_header\":\n",
    "#                 break\n",
    "\n",
    "#         # Parse header to determine number of vertices and property order.\n",
    "#         num_vertices = 0\n",
    "#         properties = []\n",
    "#         for line in header_lines:\n",
    "#             if line.startswith(\"element vertex\"):\n",
    "#                 num_vertices = int(line.split()[2])\n",
    "#             elif line.startswith(\"property\"):\n",
    "#                 tokens = line.split()\n",
    "#                 if len(tokens) >= 3:\n",
    "#                     properties.append(tokens[2])\n",
    "\n",
    "#         # The first 3 properties are the means (x,y,z)\n",
    "#         idx = 0\n",
    "#         means_idx = list(range(idx, idx + 3))\n",
    "#         idx += 3\n",
    "\n",
    "#         # Next 3 are normals (unused)\n",
    "#         normals_idx = list(range(idx, idx + 3))\n",
    "#         idx += 3\n",
    "\n",
    "#         # Next block: either color values or SH coefficients until we hit \"opacity\".\n",
    "#         try:\n",
    "#             opacity_index = properties.index(\"opacity\")\n",
    "#         except ValueError:\n",
    "#             raise ValueError(\"Header does not contain an 'opacity' property.\")\n",
    "        \n",
    "#         color_props = properties[idx:opacity_index]\n",
    "\n",
    "#         # If any property in the block starts with \"f_rest\", we assume SH coefficients.\n",
    "#         if any(prop.startswith(\"f_rest\") for prop in color_props):\n",
    "#             mode = \"sh\"\n",
    "#         else:\n",
    "#             mode = \"colors\"\n",
    "\n",
    "#         if mode == \"colors\":\n",
    "#             # Plain colors: the number of floats equals D.\n",
    "#             color_indices = list(range(idx, opacity_index))\n",
    "#         else:\n",
    "#             # SH coefficients: expect two blocks of equal length.\n",
    "#             total_color_floats = len(color_props)\n",
    "#             if total_color_floats % 2 != 0:\n",
    "#                 raise ValueError(\"SH coefficients block should contain an even number of floats.\")\n",
    "#             M = total_color_floats // 2  # floats per block\n",
    "#             if M % 3 != 0:\n",
    "#                 raise ValueError(\"The number of SH coefficient floats in one block is not a multiple of 3.\")\n",
    "#             K = M // 3  # Number of SH bands\n",
    "#             sh0_indices = list(range(idx, idx + M))\n",
    "#             shN_indices = list(range(idx + M, idx + 2 * M))\n",
    "#         idx = opacity_index  # Move index to opacity property\n",
    "\n",
    "#         # Opacity: one float per vertex.\n",
    "#         opacity_idx = idx\n",
    "#         idx += 1\n",
    "\n",
    "#         # Next properties: scales (prefixed with \"scale_\")\n",
    "#         scale_indices = []\n",
    "#         while idx < len(properties) and properties[idx].startswith(\"scale_\"):\n",
    "#             scale_indices.append(idx)\n",
    "#             idx += 1\n",
    "\n",
    "#         # Finally: rotations (quaternions; properties starting with \"rot_\")\n",
    "#         rot_indices = []\n",
    "#         while idx < len(properties) and properties[idx].startswith(\"rot_\"):\n",
    "#             rot_indices.append(idx)\n",
    "#             idx += 1\n",
    "\n",
    "#         n_float = len(properties)\n",
    "#         # Read the binary data: num_vertices * (n_float floats, 4 bytes each)\n",
    "#         vertex_data = f.read(num_vertices * 4 * n_float)\n",
    "#         data = np.frombuffer(vertex_data, dtype=np.float32).reshape(num_vertices, n_float)\n",
    "\n",
    "#         # Extract means.\n",
    "#         means = data[:, means_idx]\n",
    "\n",
    "#         if mode == \"colors\":\n",
    "#             # Recover colors: inverse of the stored transformation.\n",
    "#             # Stored as: f_dc = (color - 0.5) / 0.2820947917738781.\n",
    "#             f_dc = data[:, color_indices]\n",
    "#             colors = f_dc * 0.2820947917738781 + 0.5\n",
    "#         else:\n",
    "#             # SH coefficients: recover both sets and reshape to [N, K, 3].\n",
    "#             sh0 = data[:, sh0_indices]\n",
    "#             shN = data[:, shN_indices]\n",
    "#             colors_sh0 = sh0.reshape(num_vertices, K, 3)\n",
    "#             colors_shN = shN.reshape(num_vertices, K, 3)\n",
    "\n",
    "#         opacities = data[:, opacity_idx]\n",
    "#         scales = data[:, scale_indices]\n",
    "#         quats = data[:, rot_indices]\n",
    "\n",
    "#         # Convert to torch tensors.\n",
    "#         gaussians = {\n",
    "#             \"means\": torch.from_numpy(means),\n",
    "#             \"opacities\": torch.from_numpy(opacities),\n",
    "#             \"scales\": torch.from_numpy(scales),\n",
    "#             \"quats\": torch.from_numpy(quats)\n",
    "#         }\n",
    "#         if mode == \"colors\":\n",
    "#             gaussians[\"colors\"] = torch.from_numpy(colors)\n",
    "#         else:\n",
    "#             gaussians[\"sh0\"] = torch.from_numpy(colors_sh0)\n",
    "#             gaussians[\"shN\"] = torch.from_numpy(colors_shN)\n",
    "\n",
    "#         return gaussians\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ply_as_parameters(\n",
    "    file_path: str,\n",
    "    sh_degree: int = 3,\n",
    ") -> torch.nn.ParameterDict:\n",
    "    \"\"\"\n",
    "    PLYファイルを読み込み、公式ドキュメントに基づく以下のパラメータを\n",
    "    torch.nn.ParameterDict 形式で返す:\n",
    "      - means     (N, 3)\n",
    "      - quats     (N, 4)   [w, x, y, z]\n",
    "      - scales    (N, 3)\n",
    "      - opacities (N,)\n",
    "      - colors    (N, K, 3)   # K = (sh_degree+1)**2, ここでは SH係数と想定\n",
    "\n",
    "    前提:\n",
    "      1行 (頂点) あたり以下の順序で 62 個の float を持つ:\n",
    "        x, y, z,\n",
    "        nx, ny, nz,\n",
    "        f_dc_0..2 + f_rest_0..44 (計48個),\n",
    "        opacity,\n",
    "        scale_0..2,\n",
    "        rot_0..3\n",
    "\n",
    "      3 + 3 + 48 + 1 + 3 + 4 = 62\n",
    "\n",
    "    引数:\n",
    "      file_path : 読み込み対象の PLYファイル (binary_little_endian)\n",
    "      sh_degree : SHの最大次数 (default=3)。colors の形状を (N, K, 3) にリシェイプするために用いる。\n",
    "\n",
    "    戻り値:\n",
    "      torch.nn.ParameterDict({\n",
    "          \"means\":     (N, 3),\n",
    "          \"quats\":     (N, 4),\n",
    "          \"scales\":    (N, 3),\n",
    "          \"opacities\": (N,),\n",
    "          \"colors\":    (N, K, 3),\n",
    "      })\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 1) ヘッダーを読み込んで頂点数 num_vertices を取得\n",
    "    # ---------------------------------------------------\n",
    "    with open(file_path, 'rb') as f:\n",
    "        header_lines = []\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                # ファイルの終端など\n",
    "                break\n",
    "            line_str = line.decode('utf-8', errors='ignore').strip()\n",
    "            header_lines.append(line_str)\n",
    "            if line_str == 'end_header':\n",
    "                break\n",
    "\n",
    "        num_vertices = 0\n",
    "        for hl in header_lines:\n",
    "            if hl.startswith('element vertex'):\n",
    "                # 例: \"element vertex 899421\" のような行を想定\n",
    "                parts = hl.split()\n",
    "                num_vertices = int(parts[2])\n",
    "                break\n",
    "\n",
    "        # 2) ヘッダー以降のバイナリ部分を読み込み (float32, リトルエンディアン)\n",
    "        # 頂点数 × 62個/頂点 = 全体で (num_vertices * 62) 個の float\n",
    "        data = np.fromfile(f, dtype='<f4', count=num_vertices * 62)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3) 形を (num_vertices, 62) に整形\n",
    "    # ---------------------------------------------------\n",
    "    data = data.reshape(num_vertices, 62)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4) 項目ごとにスライスして取り出す\n",
    "    # ---------------------------------------------------\n",
    "    # means: x,y,z\n",
    "    means = data[:, 0:3]\n",
    "\n",
    "    # nx, ny, nz は今回は使わないので読み飛ばし\n",
    "    # data[:, 3:6]\n",
    "\n",
    "    # colors (SH係数として 48個)\n",
    "    #  → ex) K = (sh_degree+1)**2 (=16 if sh_degree=3)\n",
    "    #  → shape (N, 48) → (N, K, 3) = (N,16,3)\n",
    "    color_count = (sh_degree + 1)**2 * 3  # 例: 16*3=48\n",
    "    colors_raw = data[:, 6 : 6 + color_count]\n",
    "\n",
    "    # opacity\n",
    "    opacity = data[:, 6 + color_count]  # 1要素\n",
    "\n",
    "    # scales: scale_0, scale_1, scale_2\n",
    "    scales = data[:, 7 + color_count : 10 + color_count]\n",
    "\n",
    "    # quats: rot_0..3 ( w, x, y, z )\n",
    "    quats = data[:, 10 + color_count : 14 + color_count]\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 5) Torchのテンソルに変換\n",
    "    # ---------------------------------------------------\n",
    "    means_t   = torch.from_numpy(means)         # (N, 3)\n",
    "    quats_t   = torch.from_numpy(quats)         # (N, 4)\n",
    "    scales_t  = torch.from_numpy(scales)        # (N, 3)\n",
    "    opacity_t = torch.from_numpy(opacity)       # (N,)\n",
    "\n",
    "    # colors\n",
    "    #  shape (N, 48) → reshape (N, K, 3)\n",
    "    K = (sh_degree + 1)**2\n",
    "    colors_reshaped = colors_raw.reshape(num_vertices, K, 3)\n",
    "    colors_t = torch.from_numpy(colors_reshaped)  # (N, K, 3)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 6) nn.ParameterDict にまとめる\n",
    "    # ---------------------------------------------------\n",
    "    splats = torch.nn.ParameterDict({\n",
    "        \"means\":     torch.nn.Parameter(means_t),\n",
    "        \"quats\":     torch.nn.Parameter(quats_t),\n",
    "        \"scales\":    torch.nn.Parameter(scales_t),\n",
    "        \"opacities\": torch.nn.Parameter(opacity_t),\n",
    "        \"colors\":    torch.nn.Parameter(colors_t),\n",
    "    })\n",
    "\n",
    "    return splats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterDict(\n",
      "    (colors): Parameter containing: [torch.FloatTensor of size 899421x16x3]\n",
      "    (means): Parameter containing: [torch.FloatTensor of size 899421x3]\n",
      "    (opacities): Parameter containing: [torch.FloatTensor of size 899421]\n",
      "    (quats): Parameter containing: [torch.FloatTensor of size 899421x4]\n",
      "    (scales): Parameter containing: [torch.FloatTensor of size 899421x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gaussians = load_ply_as_parameters(file_path)\n",
    "print(gaussians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Transforms...\n",
      "64 training cameras\n",
      "0 test cameras\n",
      "uid= 0\n",
      "R= [[ 1. -0. -0.]\n",
      " [ 0. -1. -0.]\n",
      " [ 0. -0. -1.]]\n",
      "T= [ 16.804981 -16.804981  20.      ]\n",
      "FovX(deg)= 39.597755335771296\n",
      "FovY(deg)= 39.597755335771296\n",
      "image_name= 0000\n",
      "image size= (1600, 1600)\n",
      "---\n",
      "uid= 1\n",
      "R= [[ 1. -0. -0.]\n",
      " [ 0. -1. -0.]\n",
      " [ 0. -0. -1.]]\n",
      "T= [ 16.804981 -12.603736  20.      ]\n",
      "FovX(deg)= 39.597755335771296\n",
      "FovY(deg)= 39.597755335771296\n",
      "image_name= 0001\n",
      "image size= (1600, 1600)\n",
      "---\n",
      "uid= 2\n",
      "R= [[ 1. -0. -0.]\n",
      " [ 0. -1. -0.]\n",
      " [ 0. -0. -1.]]\n",
      "T= [16.804981 -8.402491 20.      ]\n",
      "FovX(deg)= 39.597755335771296\n",
      "FovY(deg)= 39.597755335771296\n",
      "image_name= 0002\n",
      "image size= (1600, 1600)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# render\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "means     = gaussians[\"means\"].to(device)\n",
    "quats     = gaussians[\"quats\"].to(device)\n",
    "scales    = gaussians[\"scales\"].to(device)\n",
    "opacities = gaussians[\"opacities\"].to(device)\n",
    "colors    = gaussians[\"colors\"].to(device)  # (N,16,3) など\n",
    "\n",
    "# viewmats  = torch.eye(4, device=device)[None, :, :]\n",
    "# Ks        = torch.tensor([\n",
    "#    [300., 0., 150.],\n",
    "#    [0., 300., 100.],\n",
    "#    [0.,   0.,   1.]], device=device)[None, :, :]\n",
    "\n",
    "# width, height = 300, 200\n",
    "\n",
    "from mine.read_transformers import readNerfSyntheticCameras, fov2focal\n",
    "from pathlib import Path\n",
    "\n",
    "nerf_dataset_path = os.path.abspath(os.path.join(\"..\", \"data\", \"transformers\"))\n",
    "\n",
    "train_cams, test_cams = readNerfSyntheticCameras(nerf_dataset_path, white_background=True, extension=\".png\", eval_mode=True)\n",
    "\n",
    "print(len(train_cams), \"training cameras\")\n",
    "print(len(test_cams), \"test cameras\")\n",
    "\n",
    "# 例: 取り出し\n",
    "for cam in train_cams[:3]:\n",
    "    print(\"uid=\", cam.uid)\n",
    "    print(\"R=\", cam.R)\n",
    "    print(\"T=\", cam.T)\n",
    "    print(\"FovX(deg)=\", np.degrees(cam.FovX))\n",
    "    print(\"FovY(deg)=\", np.degrees(cam.FovY))\n",
    "    print(\"image_name=\", cam.image_name)\n",
    "    print(\"image size=\", (cam.width, cam.height))\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =\n",
      " [[555.55554   0.      200.     ]\n",
      " [  0.      555.55554 200.     ]\n",
      " [  0.        0.        1.     ]]\n"
     ]
    }
   ],
   "source": [
    "# 0番目カメラを取り出す\n",
    "cam = train_cams[0]\n",
    "width, height = cam.width/4, cam.height/4\n",
    "\n",
    "# f_x, f_y を計算\n",
    "f_x = fov2focal(cam.FovX, width)   # 横幅に対する焦点距離\n",
    "f_y = fov2focal(cam.FovY, height)  # 縦幅に対する焦点距離\n",
    "\n",
    "# 光学中心 cx, cy (画像中心と仮定)\n",
    "cx = width  / 2.0\n",
    "cy = height / 2.0\n",
    "\n",
    "# Intrinsics 行列\n",
    "K = np.array([\n",
    "    [f_x,   0,   cx],\n",
    "    [  0, f_y,   cy],\n",
    "    [  0,   0,   1 ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\"K =\\n\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2C =\n",
      " [[  1.        -0.        -0.        16.804981]\n",
      " [  0.        -1.        -0.       -16.804981]\n",
      " [  0.        -0.        -1.        20.      ]\n",
      " [  0.         0.         0.         1.      ]]\n"
     ]
    }
   ],
   "source": [
    "# 4x4 のワールド→カメラ変換 (Extrinsic) を作る\n",
    "W2C = np.eye(4, dtype=np.float32)\n",
    "W2C[:3, :3] = cam.R  # cam.R はすでに転置済みの場合あり; それは readNerfSyntheticInfo の仕様\n",
    "W2C[:3,  3] = cam.T\n",
    "\n",
    "print(\"W2C =\\n\", W2C)\n",
    "\n",
    "# 内部パラメータ\n",
    "K_3x3 = K[:3, :3]  # 上で計算したintrinsics, 3x3部だけ使う\n",
    "\n",
    "# ライブラリに合った形に reshape/unsqueeze するなど\n",
    "viewmats = torch.from_numpy(W2C)[None, :, :].to(device)  # 1カメラ分の4x4行列をCUDAへ\n",
    "Ks       = torch.from_numpy(K_3x3)[None, :, :].to(device) # 1カメラ分の3x3行列をCUDAへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fully_fused_projection_packed_fwd(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch.Tensor, arg1: Optional[torch.Tensor], arg2: Optional[torch.Tensor], arg3: Optional[torch.Tensor], arg4: torch.Tensor, arg5: torch.Tensor, arg6: int, arg7: int, arg8: float, arg9: float, arg10: float, arg11: float, arg12: bool, arg13: gsplat.csrc.CameraModelType) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n\nInvoked with: tensor([[-68.5901, -69.1323, -10.7565],\n        [-68.5901, -68.5901, -10.7556],\n        [-69.1323, -68.5901, -10.7532],\n        ...,\n        [ 22.3874,  -5.4701,  -8.4596],\n        [-14.2768,   3.5733, -10.0789],\n        [ -4.2468,  21.4005, -10.3054]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), None, tensor([[ 1.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.0000,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.7524, -0.2644,  0.0622,  0.6380],\n        [ 0.2688,  0.3703,  0.4453, -0.4253],\n        [ 1.4042,  0.4811, -0.3420,  0.1014]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), tensor([[ -0.6121,  -0.6121,  -0.6121],\n        [ -0.6121,  -0.6121,  -0.6121],\n        [ -0.6121,  -0.6121,  -0.6121],\n        ...,\n        [ -1.2028,  -1.0000, -11.0321],\n        [ -2.8188,  -1.7318,  -2.7828],\n        [ -1.4473,  -1.0357, -29.0351]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), tensor([[[  1.0000,  -0.0000,  -0.0000,  16.8050],\n         [  0.0000,  -1.0000,  -0.0000, -16.8050],\n         [  0.0000,  -0.0000,  -1.0000,  20.0000],\n         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), tensor([[[555.5555,   0.0000, 200.0000],\n         [  0.0000, 555.5555, 200.0000],\n         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 400.0, 400.0, 0.3, 0.01, 10000000000.0, 0.0, False, <CameraModelType.PINHOLE: 0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rgb_image, alpha, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mrasterization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopacities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mviewmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msh_degree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpinhole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/try-papers/gsplat/gsplat/rendering.py:297\u001b[0m, in \u001b[0;36mrasterization\u001b[0;34m(means, quats, scales, opacities, colors, viewmats, Ks, width, height, near_plane, far_plane, radius_clip, eps2d, sh_degree, packed, tile_size, backgrounds, render_mode, sparse_grad, absgrad, rasterize_mode, channel_chunk, distributed, camera_model, covars)\u001b[0m\n\u001b[1;32m    294\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(viewmats)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Project Gaussians to 2D. Directly pass in {quats, scales} is faster than precomputing covars.\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m proj_results \u001b[38;5;241m=\u001b[39m \u001b[43mfully_fused_projection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcovars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mviewmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnear_plane\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnear_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfar_plane\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfar_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalc_compensations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrasterize_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mantialiased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamera_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packed:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# The results are packed into shape [nnz, ...]. All elements are valid.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     (\n\u001b[1;32m    319\u001b[0m         camera_ids,\n\u001b[1;32m    320\u001b[0m         gaussian_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m         compensations,\n\u001b[1;32m    326\u001b[0m     ) \u001b[38;5;241m=\u001b[39m proj_results\n",
      "File \u001b[0;32m~/try-papers/gsplat/gsplat/cuda/_wrapper.py:306\u001b[0m, in \u001b[0;36mfully_fused_projection\u001b[0;34m(means, covars, quats, scales, viewmats, Ks, width, height, eps2d, near_plane, far_plane, radius_clip, packed, sparse_grad, calc_compensations, camera_model)\u001b[0m\n\u001b[1;32m    304\u001b[0m Ks \u001b[38;5;241m=\u001b[39m Ks\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packed:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FullyFusedProjectionPacked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mviewmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnear_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfar_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mradius_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalc_compensations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcamera_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _FullyFusedProjection\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    325\u001b[0m         means,\n\u001b[1;32m    326\u001b[0m         covars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m         camera_model,\n\u001b[1;32m    339\u001b[0m     )\n",
      "File \u001b[0;32m~/try-papers/gsplat/venv/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/try-papers/gsplat/gsplat/cuda/_wrapper.py:1066\u001b[0m, in \u001b[0;36m_FullyFusedProjectionPacked.forward\u001b[0;34m(ctx, means, covars, quats, scales, viewmats, Ks, width, height, eps2d, near_plane, far_plane, radius_clip, sparse_grad, calc_compensations, camera_model)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1036\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     camera_model: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinhole\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mortho\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfisheye\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinhole\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1052\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m   1053\u001b[0m     camera_model_type \u001b[38;5;241m=\u001b[39m _make_lazy_cuda_obj(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCameraModelType.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcamera_model\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m     (\n\u001b[1;32m   1058\u001b[0m         indptr,\n\u001b[1;32m   1059\u001b[0m         camera_ids,\n\u001b[1;32m   1060\u001b[0m         gaussian_ids,\n\u001b[1;32m   1061\u001b[0m         radii,\n\u001b[1;32m   1062\u001b[0m         means2d,\n\u001b[1;32m   1063\u001b[0m         depths,\n\u001b[1;32m   1064\u001b[0m         conics,\n\u001b[1;32m   1065\u001b[0m         compensations,\n\u001b[0;32m-> 1066\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_make_lazy_cuda_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfully_fused_projection_packed_fwd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional\u001b[39;49;00m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional\u001b[39;49;00m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional\u001b[39;49;00m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mviewmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnear_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfar_plane\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mradius_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalc_compensations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcamera_model_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m calc_compensations:\n\u001b[1;32m   1083\u001b[0m         compensations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/try-papers/gsplat/gsplat/cuda/_wrapper.py:14\u001b[0m, in \u001b[0;36m_make_lazy_cuda_func.<locals>.call_cuda\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_cuda\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _C\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: fully_fused_projection_packed_fwd(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch.Tensor, arg1: Optional[torch.Tensor], arg2: Optional[torch.Tensor], arg3: Optional[torch.Tensor], arg4: torch.Tensor, arg5: torch.Tensor, arg6: int, arg7: int, arg8: float, arg9: float, arg10: float, arg11: float, arg12: bool, arg13: gsplat.csrc.CameraModelType) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n\nInvoked with: tensor([[-68.5901, -69.1323, -10.7565],\n        [-68.5901, -68.5901, -10.7556],\n        [-69.1323, -68.5901, -10.7532],\n        ...,\n        [ 22.3874,  -5.4701,  -8.4596],\n        [-14.2768,   3.5733, -10.0789],\n        [ -4.2468,  21.4005, -10.3054]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), None, tensor([[ 1.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.0000,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.7524, -0.2644,  0.0622,  0.6380],\n        [ 0.2688,  0.3703,  0.4453, -0.4253],\n        [ 1.4042,  0.4811, -0.3420,  0.1014]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), tensor([[ -0.6121,  -0.6121,  -0.6121],\n        [ -0.6121,  -0.6121,  -0.6121],\n        [ -0.6121,  -0.6121,  -0.6121],\n        ...,\n        [ -1.2028,  -1.0000, -11.0321],\n        [ -2.8188,  -1.7318,  -2.7828],\n        [ -1.4473,  -1.0357, -29.0351]], device='cuda:0',\n       grad_fn=<ToCopyBackward0>), tensor([[[  1.0000,  -0.0000,  -0.0000,  16.8050],\n         [  0.0000,  -1.0000,  -0.0000, -16.8050],\n         [  0.0000,  -0.0000,  -1.0000,  20.0000],\n         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), tensor([[[555.5555,   0.0000, 200.0000],\n         [  0.0000, 555.5555, 200.0000],\n         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 400.0, 400.0, 0.3, 0.01, 10000000000.0, 0.0, False, <CameraModelType.PINHOLE: 0>"
     ]
    }
   ],
   "source": [
    "\n",
    "rgb_image, alpha, metadata = rasterization(\n",
    "    means,\n",
    "    quats,\n",
    "    scales,\n",
    "    opacities,\n",
    "    colors,\n",
    "    viewmats,\n",
    "    Ks,\n",
    "    width,\n",
    "    height,\n",
    "    sh_degree=3,\n",
    "    tile_size=64,\n",
    "    channel_chunk=1,\n",
    "    sparse_grad=True,\n",
    "    camera_model=\"pinhole\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 299.5, 199.5, -0.5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the rendered image.\n",
    "plt.imshow(rgb_image.squeeze().cpu().detach().numpy())\n",
    "plt.axis(\"off\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
